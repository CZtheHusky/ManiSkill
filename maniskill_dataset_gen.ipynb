{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternvl_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternvl_eval_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_logits_processor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/workspace/vlav-project/train_stack_cube100/internvl2-2b/v2-20250813-084855/checkpoint-930\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from internvl_eval.internvl_eval_agent import prepare_logits_processor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/workspace/vlav-project/train_stack_cube100/internvl2-2b/v2-20250813-084855/checkpoint-930\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy_path = \"stack_cubes/images\"\n",
    "# import os\n",
    "# for filen in os.listdir(legacy_path):\n",
    "#     if filen.endswith(\".jpg\") and \"_\" in filen:\n",
    "#         os.remove(os.path.join(legacy_path, filen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/sapien/_vulkan_tricks.py:21: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n",
      "/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find Vulkan ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
      "  warn(\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from internvl_eval.utils import ManiSkillTrajectoryDataset, InternVLPretrainDatasetGenerator\n",
    "\n",
    "# joint_dataset = ManiSkillTrajectoryDataset(dataset_file=\"demos/StackCube-v1/motionplanning/trajectory.rgb.pd_joint_delta_pos.physx_cpu.h5\", success_only=False, device=None, load_count=100, is_episode_dataset=True)\n",
    "# eef_dataset = ManiSkillTrajectoryDataset(dataset_file=\"/mnt/nfs3/caozhe/workspace/ManiSkill/demos/StackCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5\", success_only=False, device=None, load_count=100, is_episode_dataset=True)\n",
    "push_dataset = ManiSkillTrajectoryDataset(dataset_file=\"demos/PushCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5\", success_only=False, device=None, load_count=100, is_episode_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InternVLPretrainDatasetGenerator(\n",
    "    dataset=push_dataset,\n",
    "    save_path=\"push_cubes\",\n",
    "    horizon=1,\n",
    "    dual_camera=False,\n",
    "    is_joint_action=False,\n",
    "    env_id=\"PushCube-v1\"\n",
    "    )\n",
    "# generator.cal_statistics()\n",
    "generator.traj_generation(filter_zero=True)\n",
    "# generator.generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:19<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "joint_dataset = ManiSkillTrajectoryDataset(dataset_file=\"demos/StackCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5\", success_only=False, device=None, load_count=100, is_episode_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InternVLPretrainDatasetGenerator(\n",
    "    dataset=joint_dataset,\n",
    "    save_path=\"stack_cubes\",\n",
    "    horizon=1,\n",
    "    dual_camera=True,\n",
    "    dual_camera=True,\n",
    "    is_joint_action=False,\n",
    "    env_id=\"StackCube-v1\"\n",
    "    )\n",
    "# generator.cal_statistics()\n",
    "generator.traj_generation(filter_zero=True)\n",
    "# generator.generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from mani_skill.utils import common\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "import imageio\n",
    "import mani_skill.envs\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "import re\n",
    "from transformers.generation.logits_process import PrefixConstrainedLogitsProcessor, LogitsProcessor\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "\n",
    "def extract_action_vector(llm_output: str):\n",
    "    \"\"\"\n",
    "    从 LLM 的输出字符串中提取一个 7D 动作向量。\n",
    "\n",
    "    Args:\n",
    "        llm_output: LLM 的字符串输出。\n",
    "                    示例: \"action: {x: -129mm, y: 442mm, z: 183mm, roll: 2 degrees, pitch: 40 degrees, yaw: 0 degrees, open: 1}\"\n",
    "                    绝对动作示例: \"action: {x: -11mm, y: 20mm, z: 33mm, quat: 10, 20, 30, 22, open: 1}\"\n",
    "\n",
    "    Returns:\n",
    "        对于相对动作 (is_abs=False)，返回一个元组 (delta_trans, delta_r, gripper_open)。\n",
    "        对于绝对动作 (is_abs=True)，返回一个元组 (abs_trans, abs_quat, gripper_open)。\n",
    "        如果无法找到所有值，则返回 None。\n",
    "    \"\"\"\n",
    "    # Define the keys we want to extract in the desired final order\n",
    "    keys_in_order = ['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'open']\n",
    "    \n",
    "    # This regex pattern finds all key-value pairs we are interested in.\n",
    "    # It captures the key (e.g., 'x') and its corresponding numeric value.\n",
    "    pattern = r\"(x|y|z|roll|pitch|yaw|open):\\s*(-?[\\d.]+)\"\n",
    "    \n",
    "    # re.findall will return a list of tuples, e.g., [('x', '-129'), ('y', '442'), ...]\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    \n",
    "    # Convert the list of tuples into a dictionary for easy lookup.\n",
    "    # The captured value is converted to a float.\n",
    "    data_dict = {key: float(value) for key, value in matches}\n",
    "    \n",
    "    # Assemble the vector in the correct order using the dictionary.\n",
    "    # We use .get() to safely handle cases where a key might be missing.\n",
    "    try:\n",
    "        vector = np.array([data_dict[key] for key in keys_in_order], dtype=np.float32)\n",
    "        return vector\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing key {e} in the LLM output: {llm_output}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def write_instruction_action(instruction: str, rgb: np.ndarray, action: str = None, raw_action: str = None):\n",
    "    \"\"\"\n",
    "    在图片上方增加一个白色背景条，并写入 instruction。\n",
    "    如果提供了 action 参数，则会在 instruction 下方额外写入一行 action。\n",
    "\n",
    "    :param instruction: 要显示的第一行指令文本。\n",
    "    :param rgb: 输入的原始图像 (numpy array)。\n",
    "    :param action: (可选) 要在第二行显示的动作文本。\n",
    "    :return: 带有文本的新图像。\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "    text_color = (0, 0, 0)  # 黑色字体\n",
    "    bg_color = (255, 255, 255)  # 白色背景\n",
    "    \n",
    "    # --- 动态计算所需空间 ---\n",
    "    texts_to_draw = [instruction]\n",
    "    if action is not None:\n",
    "        texts_to_draw.append(action)\n",
    "    if raw_action is not None:\n",
    "        texts_to_draw.append(raw_action)\n",
    "\n",
    "    # 获取每行文本的尺寸\n",
    "    text_sizes = [cv2.getTextSize(text, font, font_scale, font_thickness)[0] for text in texts_to_draw]\n",
    "    text_heights = [size[1] for size in text_sizes]\n",
    "\n",
    "    # 定义边距和行间距\n",
    "    top_margin = 10\n",
    "    bottom_margin = 10\n",
    "    line_spacing = 5 # 两行文字之间的额外间距\n",
    "\n",
    "    # 计算总的 padding 高度\n",
    "    total_text_height = sum(text_heights)\n",
    "    if len(texts_to_draw) > 1:\n",
    "        total_text_height += line_spacing * (len(texts_to_draw) - 1)\n",
    "    \n",
    "    pad_top = total_text_height + top_margin + bottom_margin\n",
    "\n",
    "    # --- 创建并绘制新图像 ---\n",
    "    h, w, _ = rgb.shape\n",
    "    new_img = np.full((h + pad_top, w, 3), bg_color, dtype=np.uint8)\n",
    "\n",
    "    # 把原图粘贴到新图像的下方\n",
    "    new_img[pad_top:, :] = rgb\n",
    "\n",
    "    # --- 逐行写入文本 ---\n",
    "    current_y = top_margin\n",
    "    for i, text in enumerate(texts_to_draw):\n",
    "        text_h = text_heights[i]\n",
    "        # 计算文本基线的 y 坐标 (putText 的 y 坐标是基线位置)\n",
    "        text_y = current_y + text_h\n",
    "        text_x = 10  # 左边距\n",
    "\n",
    "        cv2.putText(new_img, text, (text_x, text_y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # 更新下一行文本的起始 y 坐标\n",
    "        current_y = text_y + line_spacing\n",
    "\n",
    "    return new_img\n",
    "\n",
    "def write_terminal(frame, word):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 3\n",
    "    font_thickness = 4\n",
    "    text_color = (255, 0, 0)  # 红色字体\n",
    "    # 获取原始图片尺寸\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # 计算文本大小\n",
    "    (text_width, text_height), _ = cv2.getTextSize(word, font, font_scale, font_thickness)\n",
    "\n",
    "    # 设置文本位置为中心\n",
    "    text_x = (w - text_width) // 2\n",
    "    text_y = (h + text_height) // 2\n",
    "\n",
    "    # 在图像上写入文本\n",
    "    cv2.putText(frame, word, (text_x, text_y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "class VideoRecorder:\n",
    "    def __init__(self, save_path, fps=20, num_envs=1):\n",
    "        self.save_path = save_path\n",
    "        self.fps = fps\n",
    "        self.env_id_num = defaultdict(int)\n",
    "        self.recorder = defaultdict(list)\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=32)\n",
    "        self.futures = []\n",
    "        self.venv_reward = defaultdict(float)\n",
    "        self.num_envs = num_envs\n",
    "        self.done_nums = 0\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "        \n",
    "    def _check_futures(self):\n",
    "        done, _ = concurrent.futures.wait(self.futures, timeout=0)\n",
    "        for future in done:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Video save failed: {e}\")\n",
    "        self.futures = [f for f in self.futures if not f.done()]        \n",
    "\n",
    "    def append_obs(self, cameras: List[np.ndarray], rewards: List[float], terminated_status: np.ndarray, actions=None, raw_actions=None):\n",
    "        if actions is None:\n",
    "            actions = [None] * self.num_envs\n",
    "        if raw_actions is None:\n",
    "            raw_actions = [None] * self.num_envs\n",
    "        metrics = {}\n",
    "        for env_id in range(self.num_envs):\n",
    "            reward = rewards[env_id]\n",
    "            is_terminated = terminated_status[env_id]\n",
    "            action = actions[env_id]\n",
    "            raw_action = raw_actions[env_id]\n",
    "            camera = [cam[env_id] for cam in cameras]\n",
    "            camera = camera[0] if len(camera) == 1 else np.concatenate(camera, axis=1)    \n",
    "            # print(camera.shape, \"camera\")\n",
    "            self.venv_reward[env_id] += reward\n",
    "            # print(reward)\n",
    "            instruction = f\"Rew: {reward} EPR: {self.venv_reward[env_id]}\"\n",
    "            raw_action = f\"RA: {action_to_str(raw_action, 3)}\" if raw_action is not None else None\n",
    "            action = f\"A: {action_to_str(action, 3)}\" if action is not None else None\n",
    "            camera = write_instruction_action(instruction, camera, action, raw_action)\n",
    "            # print(camera.shape, \"inst\")\n",
    "            self.recorder[env_id].append(camera)       \n",
    "            if is_terminated:\n",
    "                file_name = f\"{self.done_nums}_{env_id}_{self.env_id_num[env_id]}_{len(self.recorder[env_id])}_{self.venv_reward[env_id]}.mp4\"\n",
    "                save_path = f\"{self.save_path}/{file_name}\"\n",
    "                print(f\"Writing video: {save_path}, Num steps: {len(self.recorder[env_id])}, Reward: {self.venv_reward[env_id]}\")\n",
    "                future = self.executor.submit(\n",
    "                    video_writing, \n",
    "                    [self.recorder[env_id]], \n",
    "                    save_path, \n",
    "                    fps=self.fps\n",
    "                )\n",
    "                self.futures.append(future)\n",
    "                self.env_id_num[env_id] += 1\n",
    "                metrics[self.done_nums] = {\n",
    "                    \"env_id\": env_id,\n",
    "                    'num_steps': len(self.recorder[env_id]),\n",
    "                    \"sparse_reward\": self.venv_reward[env_id],\n",
    "                    \"file_name\": file_name\n",
    "                }    \n",
    "                self.recorder[env_id] = []    \n",
    "                self.venv_reward[env_id] = 0\n",
    "                self.done_nums += 1\n",
    "        self._check_futures()  # 每次添加新任务后检查已完成的任务\n",
    "        return metrics\n",
    "          \n",
    "    def close(self):\n",
    "        print(\"Closing VideoRecorder, waiting for pending video writes to finish...\")\n",
    "        # shutdown(wait=True) 会阻止新任务提交，并等待所有已提交任务完成\n",
    "        self.executor.shutdown(wait=True)\n",
    "        # 最后再检查一次，确保捕获所有任务的异常\n",
    "        self._check_futures()\n",
    "        print(\"All video writing tasks are complete.\")\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    if isinstance(image_file, str):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    elif isinstance(image_file, np.ndarray):\n",
    "        image = Image.fromarray(image_file)\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "def read_jsonl_standard(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    使用 Python 标准库逐行读取 JSONL 文件。\n",
    "    \n",
    "    :param file_path: JSONL 文件的路径。\n",
    "    :return: 一个包含所有解析后的JSON对象（字典）的列表。\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # 移除行尾可能存在的空白字符（包括换行符）\n",
    "                clean_line = line.strip()\n",
    "                if clean_line:  # 确保不是空行\n",
    "                    # 解析当前行\n",
    "                    data.append(json.loads(clean_line))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件未找到于 '{file_path}'\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"错误：文件 '{file_path}' 中存在JSON解析错误: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def append_to_jsonl(new_data, filename='log.jsonl'):\n",
    "    \"\"\"向JSON Lines文件追加一条新记录。\"\"\"\n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        # 将字典转换为JSON字符串，并在末尾添加换行符\n",
    "        f.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_action_vectors(s: str):\n",
    "    results = []\n",
    "    segments = s.strip().split('|')\n",
    "    for seg in segments:\n",
    "        seg = seg.strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "        try:\n",
    "            # 用正则匹配形如 \"+0 -8 -2 +3 -4 +13 +1\" 的 7 个有符号整数\n",
    "            matches = re.findall(r'[+-]?\\d+', seg)\n",
    "            if len(matches) != 7:\n",
    "                results.append(None)\n",
    "            else:\n",
    "                results.append(np.array([int(m) for m in matches]))\n",
    "        except:\n",
    "            results.append(None)\n",
    "    return results\n",
    "\n",
    "def extract_action_vectors(s, vector_length, expected_count=None):\n",
    "    \"\"\"\n",
    "    从字符串 s 中提取动作向量。\n",
    "    \n",
    "    参数:\n",
    "        s (str): 包含若干用 {} 括起来的动作向量字符串。\n",
    "        vector_length (int): 每个动作向量应包含的整数个数。\n",
    "        expected_count (int, optional): 期望动作数量，如果提供且不匹配则返回 None。\n",
    "        \n",
    "    返回:\n",
    "        如果 expected_count 不为 None 且与实际提取的动作数不符，则返回 None；\n",
    "        否则返回长度为提取动作数的列表，列表中每个元素要么是长度为 vector_length 的整数列表，要么是 None（表示该动作格式不合规）。\n",
    "    \"\"\"\n",
    "    contents = re.findall(r'\\{([^}]*)\\}', s)\n",
    "    pattern = re.compile(r'^-?\\d+(?:\\s+-?\\d+){' + str(vector_length - 1) + r'}$')\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        raw = content.strip()\n",
    "        if pattern.match(raw):\n",
    "            nums = list(map(int, raw.split()))\n",
    "            results.append(np.array(nums))\n",
    "        else:\n",
    "            print(f\"Invalid action format: '{raw}'\")\n",
    "            break\n",
    "    if len(results) > expected_count:\n",
    "        results = results[:expected_count]\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_and_validate_vector(input_str: str, num_len: int = 7):\n",
    "    if not isinstance(input_str, str):\n",
    "        return None\n",
    "    s = input_str.strip()\n",
    "    if s.startswith('{'):\n",
    "        s = s[1:]\n",
    "    if s.endswith(\"}\"):\n",
    "        s = s[:-1]\n",
    "    content = s\n",
    "    if not content:\n",
    "        return None\n",
    "    parts = content.split()\n",
    "    if len(parts) != num_len:\n",
    "        return None\n",
    "    try:\n",
    "        vector = [int(p) for p in parts]\n",
    "        return np.array(vector, dtype=np.float32)  \n",
    "    except ValueError:\n",
    "        print(\"Error response:\", input_str)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def video_writing(frames: List[List[np.ndarray]], save_path: str, fps: int = 10):\n",
    "    max_length = max([len(frame) for frame in frames])\n",
    "    num_in_one = len(frames)\n",
    "    # concatenate all the 16 frames of the same timestep into one frame (4*4)\n",
    "    num_per_side = np.sqrt(num_in_one).astype(int)\n",
    "    frames_concat = []\n",
    "    for i in range(max_length):\n",
    "        frame = []\n",
    "        for j in range(len(frames)):\n",
    "            if i < len(frames[j]):\n",
    "                frame.append(frames[j][i])\n",
    "            elif len(frames[j]) == 0:\n",
    "                # if the frame is empty, fill with a black frame\n",
    "                frame.append(np.zeros_like(frames[0][0]))\n",
    "            else:\n",
    "                # if the frame is not enough, fill with the last frame, with word \"Terminal\" on it \n",
    "                last_frame = frames[j][-1]\n",
    "                frame.append(write_terminal(last_frame, \"Terminated\"))\n",
    "                \n",
    "        # concatenate the frames in the same timestep into one frame (4 * 4)\n",
    "        rows = [np.concatenate(frame[i * num_per_side:(i + 1) * num_per_side], axis=1) for i in range(num_per_side)]\n",
    "        if len(rows) == 1:\n",
    "            frame_concat = rows[0]\n",
    "        else:\n",
    "            frame_concat = np.concatenate(rows, axis=0)\n",
    "        frames_concat.append(frame_concat)\n",
    "    with imageio.get_writer(save_path, fps=fps, ffmpeg_params=['-loglevel', 'error']) as writer:\n",
    "        for frame in frames_concat:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "def load_h5_data(data):\n",
    "    out = dict()\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], h5py.Dataset):\n",
    "            out[k] = data[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(data[k])\n",
    "    return out\n",
    "\n",
    "def to_tensors(x, device=None):\n",
    "    \"\"\"\n",
    "    Converts numpy arrays or dicts of numpy arrays to torch tensors.\n",
    "    If device is specified, moves the tensors to that device.\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        return {k: to_tensors(v, device) for k, v in x.items()}\n",
    "    elif isinstance(x, np.ndarray) and device is not None:\n",
    "        tensor = torch.as_tensor(x).to(device)\n",
    "        return tensor\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def action_to_str(action, num_floats: int = 4):\n",
    "    return [np.round(a, num_floats) for a in action.values()] if isinstance(action, dict) else [np.round(a, num_floats) for a in action]\n",
    "\n",
    "\n",
    "def quat_to_rpy(quaternion, degrees: bool = True):\n",
    "    rotation_object = R.from_quat([quaternion[1], quaternion[2], quaternion[3], quaternion[0]])\n",
    "    rpy = rotation_object.as_euler('xyz', degrees=degrees)\n",
    "    return rpy\n",
    "\n",
    "class ManiSkillTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
    "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
    "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
    "\n",
    "    Args:\n",
    "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
    "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
    "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
    "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_file: str, load_count=-1, success_only: bool = False, device = None, is_episode_dataset=True) -> None:\n",
    "        self.dataset_file = dataset_file\n",
    "        self.device = device\n",
    "        self.data = h5py.File(dataset_file, \"r\")\n",
    "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "        self.json_data = load_json(json_path)\n",
    "        self.episodes = self.json_data[\"episodes\"]\n",
    "        self.env_info = self.json_data[\"env_info\"]\n",
    "        self.env_id = self.env_info[\"env_id\"]\n",
    "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
    "        if isinstance(load_count, int):\n",
    "            if is_episode_dataset:\n",
    "                self.load_dataset_episode(load_count, success_only)\n",
    "            else:\n",
    "                self.load_dataset(load_count, success_only, device)\n",
    "        else:\n",
    "            pass    \n",
    "        self.is_episode_dataset = is_episode_dataset    \n",
    "        \n",
    "    def load_dataset_episode(self, load_count, success_only):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "        self.success, self.fail, self.rewards = None, None, None\n",
    "        if load_count == -1:\n",
    "            load_count = len(self.episodes)\n",
    "        for eps_id in tqdm(range(load_count)):\n",
    "            eps = self.episodes[eps_id]\n",
    "            if success_only: \n",
    "                assert \"success\" in eps, \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
    "                if not eps[\"success\"]:\n",
    "                    continue\n",
    "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
    "            trajectory = load_h5_data(trajectory)\n",
    "            eps_len = len(trajectory[\"actions\"])\n",
    "            \n",
    "            # exclude the final observation as most learning workflows do not use it\n",
    "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "            self.obs.append(obs)\n",
    "\n",
    "            self.actions.append(trajectory[\"actions\"])\n",
    "            self.terminated.append(trajectory[\"terminated\"])\n",
    "            self.truncated.append(trajectory[\"truncated\"])\n",
    "\n",
    "            # handle data that might optionally be in the trajectory\n",
    "            if \"rewards\" in trajectory:\n",
    "                if self.rewards is None:\n",
    "                    self.rewards = [trajectory[\"rewards\"]]\n",
    "                else:\n",
    "                    self.rewards.append(trajectory[\"rewards\"])\n",
    "            if \"success\" in trajectory:\n",
    "                if self.success is None:\n",
    "                    self.success = [trajectory[\"success\"]]\n",
    "                else:\n",
    "                    self.success.append(trajectory[\"success\"])\n",
    "            if \"fail\" in trajectory:\n",
    "                if self.fail is None:\n",
    "                    self.fail = [trajectory[\"fail\"]]\n",
    "                else:\n",
    "                    self.fail.append(trajectory[\"fail\"])\n",
    "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "        # uint16 dtype is used to conserve disk space and memory\n",
    "        # you can optimize this dataset code to keep it as uint16 and process that\n",
    "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "        # it can automatically be converted to torch tensors without complaint\n",
    "        for i in range(len(self.obs)):\n",
    "            self.obs[i] = remove_np_uint16(self.obs[i])\n",
    "                \n",
    "\n",
    "    def get_sequence_dataset(self):\n",
    "        import copy\n",
    "        new_dataset = copy.copy(self)\n",
    "        sequence_obs = None\n",
    "        for obs in new_dataset.obs:\n",
    "            if sequence_obs is None:\n",
    "                sequence_obs = obs\n",
    "            else:\n",
    "                sequence_obs = common.append_dict_array(sequence_obs, obs)\n",
    "        new_dataset.obs = sequence_obs\n",
    "        new_dataset.actions = np.vstack(new_dataset.actions)\n",
    "        new_dataset.terminated = np.concatenate(new_dataset.terminated)\n",
    "        new_dataset.truncated = np.concatenate(new_dataset.truncated)\n",
    "        if new_dataset.rewards is not None:\n",
    "            new_dataset.rewards = np.concatenate(new_dataset.rewards)\n",
    "        if new_dataset.success is not None:\n",
    "            new_dataset.success = np.concatenate(new_dataset.success)\n",
    "        if new_dataset.fail is not None:\n",
    "            new_dataset.fail = np.concatenate(new_dataset.fail)\n",
    "        new_dataset.is_episode_dataset = False\n",
    "        return new_dataset\n",
    "      \n",
    "        \n",
    "    def load_dataset(self, load_count, success_only, device):\n",
    "        self.obs = None\n",
    "        self.actions = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "        self.success, self.fail, self.rewards = None, None, None\n",
    "        if load_count == -1:\n",
    "            load_count = len(self.episodes)\n",
    "        for eps_id in tqdm(range(load_count)):\n",
    "            eps = self.episodes[eps_id]\n",
    "            if success_only: \n",
    "                assert \"success\" in eps, \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
    "                if not eps[\"success\"]:\n",
    "                    continue\n",
    "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
    "            trajectory = load_h5_data(trajectory)\n",
    "            eps_len = len(trajectory[\"actions\"])\n",
    "            \n",
    "            # exclude the final observation as most learning workflows do not use it\n",
    "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "            if eps_id == 0:\n",
    "                self.obs = obs\n",
    "            else:\n",
    "                self.obs = common.append_dict_array(self.obs, obs)\n",
    "\n",
    "            self.actions.append(trajectory[\"actions\"])\n",
    "            self.terminated.append(trajectory[\"terminated\"])\n",
    "            self.truncated.append(trajectory[\"truncated\"])\n",
    "\n",
    "            # handle data that might optionally be in the trajectory\n",
    "            if \"rewards\" in trajectory:\n",
    "                if self.rewards is None:\n",
    "                    self.rewards = [trajectory[\"rewards\"]]\n",
    "                else:\n",
    "                    self.rewards.append(trajectory[\"rewards\"])\n",
    "            if \"success\" in trajectory:\n",
    "                if self.success is None:\n",
    "                    self.success = [trajectory[\"success\"]]\n",
    "                else:\n",
    "                    self.success.append(trajectory[\"success\"])\n",
    "            if \"fail\" in trajectory:\n",
    "                if self.fail is None:\n",
    "                    self.fail = [trajectory[\"fail\"]]\n",
    "                else:\n",
    "                    self.fail.append(trajectory[\"fail\"])\n",
    "\n",
    "        self.actions = np.vstack(self.actions)\n",
    "        self.terminated = np.concatenate(self.terminated)\n",
    "        self.truncated = np.concatenate(self.truncated)\n",
    "        \n",
    "        if self.rewards is not None:\n",
    "            self.rewards = np.concatenate(self.rewards)\n",
    "        if self.success is not None:\n",
    "            self.success = np.concatenate(self.success)\n",
    "        if self.fail is not None:\n",
    "            self.fail = np.concatenate(self.fail)\n",
    "\n",
    "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "        \n",
    "        # uint16 dtype is used to conserve disk space and memory\n",
    "        # you can optimize this dataset code to keep it as uint16 and process that\n",
    "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "        # it can automatically be converted to torch tensors without complaint\n",
    "        self.obs = remove_np_uint16(self.obs)\n",
    "\n",
    "        if device is not None:\n",
    "            self.actions = to_tensors(self.actions, device=device)\n",
    "            self.obs = to_tensors(self.obs, device=device)\n",
    "            self.terminated = to_tensors(self.terminated, device=device)\n",
    "            self.truncated = to_tensors(self.truncated, device=device)\n",
    "            if self.rewards is not None:\n",
    "                self.rewards = to_tensors(self.rewards, device=device)\n",
    "            if self.success is not None:\n",
    "                self.success = to_tensors(self.terminated, device=device)\n",
    "            if self.fail is not None:\n",
    "                self.fail = to_tensors(self.truncated, device=device)\n",
    "                \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_episode_dataset:\n",
    "            return self.get_episode(idx)\n",
    "        else:\n",
    "            return self.get_step(idx)\n",
    "\n",
    "\n",
    "    def get_episode(self, idx):\n",
    "        return self.obs[idx], self.actions[idx], self.terminated[idx], self.truncated[idx]\n",
    "\n",
    "\n",
    "    def get_step(self, idx):\n",
    "        action = to_tensors(self.actions[idx], device=self.device)\n",
    "        obs = common.index_dict_array(self.obs, idx, inplace=False)\n",
    "\n",
    "        res = dict(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            terminated=self.terminated[idx],\n",
    "            truncated=self.truncated[idx],\n",
    "        )\n",
    "        if self.rewards is not None:\n",
    "            res.update(reward=self.rewards[idx])\n",
    "        if self.success is not None:\n",
    "            res.update(success=self.success[idx])\n",
    "        if self.fail is not None:\n",
    "            res.update(fail=self.fail[idx])\n",
    "        return res\n",
    "    \n",
    "\n",
    "def quat_to_rpy(quaternion, degrees: bool = True):\n",
    "    rotation_object = R.from_quat([quaternion[1], quaternion[2], quaternion[3], quaternion[0]])\n",
    "    rpy = rotation_object.as_euler('xyz', degrees=degrees)\n",
    "    return rpy\n",
    "\n",
    "INSTRUCTIONS = {\n",
    "    \"StackCube-v1\": \"stack the red cube on top of the green one\",\n",
    "    \"PickCube-v1\": \"pick up the red cube\",\n",
    "    'PushCube-v1': 'push the cube to the target position',\n",
    "}\n",
    "\n",
    "class InternVLPretrainDatasetGenerator:\n",
    "    def __init__(self, dataset: ManiSkillTrajectoryDataset, save_path: str, horizon=1, dual_camera=False, is_joint_action=False, env_id=\"StackCube-v1\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset generator for InternVL pretraining.\n",
    "\n",
    "        Args:\n",
    "            dataset_path (str): Path to the dataset file.\n",
    "            load_count (int): Number of trajectories to load. If -1, loads all.\n",
    "            success_only (bool): Whether to filter for successful trajectories only.\n",
    "            device: Device to load data onto (e.g., 'cpu', 'cuda').\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.save_path = save_path\n",
    "        self.img_save_path = os.path.join(save_path, \"images\")\n",
    "        os.makedirs(self.img_save_path, exist_ok=True)\n",
    "        rng = np.random.default_rng(0)\n",
    "        num_steps = np.sum([len(act) for act in dataset.actions])\n",
    "        self.val_ids = set(rng.choice(num_steps, size=num_steps // 20, replace=False))\n",
    "        self.rng = rng\n",
    "        self.rescale_array = {\n",
    "            'action': np.array([1000] * 7 + [1]) if is_joint_action else np.array([1000, 1000, 1000, 57.3, 57.3, 57.3] + [1]),\n",
    "            # 'action': np.array([1000] * 7 + [1]) if is_joint_action else np.array([1000, 1000, 1000, 1000, 1000, 1000] + [1]),\n",
    "            'qpos': np.array([1000] * 9),\n",
    "            'tcp_pose': np.array([1000] * 7)\n",
    "        }\n",
    "        self.horizon = horizon\n",
    "        self.dual_camera = dual_camera\n",
    "        self.instruction = INSTRUCTIONS[env_id]\n",
    "        \n",
    "    def cal_statistics(self):\n",
    "        statistics = {\n",
    "            'action': {},\n",
    "            'qpos': {},\n",
    "            'tcp_pose': {}\n",
    "        }\n",
    "        all_collections = {\n",
    "            'action': [],\n",
    "            'qpos': [],\n",
    "            'tcp_pose': []\n",
    "        }\n",
    "        for data in self.dataset:\n",
    "            # camera = data['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            qpos = data['obs'][\"agent\"][\"qpos\"]\n",
    "            tcp_pose = data['obs'][\"extra\"][\"tcp_pose\"]\n",
    "            rpy = quat_to_rpy(tcp_pose[3:7], degrees=True)\n",
    "            tcp_pose = np.concatenate([tcp_pose[:3], rpy])\n",
    "            action = data[\"action\"]\n",
    "            all_collections['action'].append(action)\n",
    "            all_collections['qpos'].append(qpos)\n",
    "            all_collections['tcp_pose'].append(tcp_pose)\n",
    "\n",
    "        for key in statistics.keys():\n",
    "            all_data = all_collections[key]\n",
    "            statistics[key]['mean'] = np.mean(all_data, axis=0).tolist()\n",
    "            statistics[key]['std'] = np.std(all_data, axis=0).tolist()\n",
    "            statistics[key]['min'] = np.min(all_data, axis=0).tolist()\n",
    "            statistics[key]['max'] = np.max(all_data, axis=0).tolist()\n",
    "        # print(\"Statistics calculated:\", statistics)\n",
    "        self.statistics = statistics\n",
    "        with open(os.path.join(self.save_path, 'statistics.json'), 'w') as f:\n",
    "            json.dump(statistics, f, indent=4)\n",
    "     \n",
    "    def rescale(self, data, key):\n",
    "        _tmp_data = np.round(data * self.rescale_array[key]).astype(np.int32)\n",
    "        if key == \"action\":\n",
    "            _tmp_data = np.clip(-999, 999, _tmp_data)\n",
    "        return _tmp_data\n",
    "    \n",
    "    def process_episode_data(self, episode_data, filter_zero=False):\n",
    "        infos = []\n",
    "        for i in range(len(episode_data['queries'])):\n",
    "            if self.horizon > 1:\n",
    "                if i + self.horizon > len(episode_data['queries']):\n",
    "                    responses = \" \".join(episode_data['action_strs'][i:])\n",
    "                    delta = i + self.horizon - len(episode_data['queries'])\n",
    "                    responses += f\" +0 +0 +0 +0 +0 +0 {episode_data['action_strs'][-1][-3:-1]}|\" * delta\n",
    "                else:\n",
    "                    responses = \" \".join(episode_data['action_strs'][i:i+self.horizon])\n",
    "            else:\n",
    "                responses = episode_data['action_strs'][i]\n",
    "                if filter_zero:\n",
    "                    if responses[:12] == \" 0 0 0 0 0 0\":\n",
    "                        continue\n",
    "            data_dict = {\n",
    "                'query': episode_data['queries'][i],\n",
    "                'response': responses,\n",
    "                'images': episode_data['cameras_save_path'][i]\n",
    "            }\n",
    "            infos.append(data_dict)\n",
    "        return infos\n",
    "\n",
    "    def traj_generation(self, filter_zero=False):\n",
    "        all_infos = []\n",
    "        val_infos = []\n",
    "        train_infos = []\n",
    "        for episode_idx, (obs, action, terminated, truncated) in enumerate(self.dataset):\n",
    "            episode_data = {\n",
    "                \"queries\": [],\n",
    "                \"action_strs\": [],\n",
    "                \"cameras_save_path\": [],\n",
    "            }\n",
    "            cameras = obs['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            if \"hand_camera\" in obs['sensor_data']:\n",
    "                hand_cameras = obs['sensor_data'][\"hand_camera\"][\"rgb\"]\n",
    "            else:\n",
    "                hand_cameras = [None] * len(cameras)\n",
    "            qposes = obs[\"agent\"][\"qpos\"]\n",
    "            tcp_pose = obs[\"extra\"][\"tcp_pose\"]\n",
    "            rescaled_qposes = self.rescale(qposes, 'qpos')\n",
    "            # rescaled_tcp_poses = self.rescale(tcp_poses, 'tcp_pose')\n",
    "            rescaled_actions = self.rescale(action, 'action')\n",
    "            rpys = []\n",
    "            for quat in tcp_pose[:, 3:7]:\n",
    "                rpy = quat_to_rpy(quat, degrees=True)\n",
    "                rpys.append(rpy)\n",
    "            rpys = np.array(rpys)\n",
    "            tcp_pose = np.concatenate([tcp_pose[:, :3], rpys], axis=-1)\n",
    "            tcp_pose[:, :3] = np.round(tcp_pose[:, :3] * 1000).astype(np.int32)\n",
    "            tcp_pose[:, 3:] = np.round(tcp_pose[:, 3:]).astype(np.int32)\n",
    "            tcp_pose = tcp_pose.astype(np.int32)\n",
    "            \n",
    "            for local_step, (camera, hand_camera, rescaled_action, rescaled_qpos, rescaled_tcp_pose) in enumerate(zip(cameras, hand_cameras, rescaled_actions, rescaled_qposes, tcp_pose)): \n",
    "                if np.all(rescaled_action[:6] == 0):\n",
    "                    continue\n",
    "                camera_save_path = os.path.join(self.img_save_path, f\"{episode_idx}_{local_step}_0.jpg\")\n",
    "                hand_camera_save_path = os.path.join(self.img_save_path, f\"{episode_idx}_{local_step}_1.jpg\")\n",
    "                camera_save_name = camera_save_path\n",
    "                hand_camera_save_name = hand_camera_save_path\n",
    "                if not os.path.exists(camera_save_path):\n",
    "                    img = Image.fromarray(camera)\n",
    "                    img.save(camera_save_path)\n",
    "                if not os.path.exists(hand_camera_save_path) and hand_camera is not None:\n",
    "                    hand_img = Image.fromarray(hand_camera)\n",
    "                    hand_img.save(hand_camera_save_path)\n",
    "                if rescaled_qpos[-1] >= 37:\n",
    "                    gripper_state = 1\n",
    "                else:\n",
    "                    gripper_state = 0\n",
    "                eef_xyz = rescaled_tcp_pose[:3]\n",
    "                eef_rpy = rescaled_tcp_pose[3:]\n",
    "                # query = f\"The current joint state of the robotic arm is as follows: {{{rescaled_qpos[0]} {rescaled_qpos[1]} {rescaled_qpos[2]} {rescaled_qpos[3]} {rescaled_qpos[4]} {rescaled_qpos[5]} {rescaled_qpos[6]} {rescaled_qpos[7]} {rescaled_qpos[8]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                # query = f\"The current position state of the robotic arm's end gripper is as follows: {{x: {eef_xyz[0]}mm, y: {eef_xyz[1]}mm, z: {eef_xyz[2]}mm, roll: {eef_rpy[0]} degrees, pitch: {eef_rpy[1]} degrees, yaw: {eef_rpy[2]} degrees, open: {gripper_state}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                # joints_str = \", \".join(f\"Joint_{i}: {v}\" for i, v in enumerate(rescaled_qpos[:8]))\n",
    "                # query = f\"The current position state of the robotic arm's end gripper is as follows: {{{joints_str}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                # action_str = \" \".join([f\"{str(int(a))}\" for a in rescaled_action])\n",
    "                rescaled_action[-1] = 0 if rescaled_action[-1] == -1 else 1\n",
    "                query = f\"The current position state of the robotic arm's end gripper is as follows: {{x: {eef_xyz[0]}mm, y: {eef_xyz[1]}mm, z: {eef_xyz[2]}mm, roll: {eef_rpy[0]} degrees, pitch: {eef_rpy[1]} degrees, yaw: {eef_rpy[2]} degrees, open: {gripper_state}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                action_str = f\"action: {{x: {rescaled_action[0]}mm, y: {rescaled_action[1]}mm, z: {rescaled_action[2]}mm, roll: {rescaled_action[3]} degrees, pitch: {rescaled_action[4]} degrees, yaw: {rescaled_action[5]} degrees, open: {rescaled_action[6]}}}\"\n",
    "                action_str = action_str\n",
    "                # if rescaled_action[0] >= 0:\n",
    "                #     action_str = \" \" + action_str\n",
    "                episode_data['queries'].append(query)\n",
    "                episode_data['action_strs'].append(action_str)\n",
    "                if self.dual_camera:\n",
    "                    camera_save_names = [camera_save_name, hand_camera_save_name]\n",
    "                else: \n",
    "                    camera_save_names = [camera_save_name]\n",
    "                episode_data['cameras_save_path'].append(camera_save_names)\n",
    "            infos = self.process_episode_data(episode_data, filter_zero)\n",
    "            all_infos.extend(infos)\n",
    "        if self.dual_camera:\n",
    "            prefix = \"dualcam_\"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "        for idx in range(len(all_infos)):\n",
    "            if idx in self.val_ids:\n",
    "                val_infos.append(all_infos[idx])\n",
    "            else:\n",
    "                train_infos.append(all_infos[idx])\n",
    "        with open(os.path.join(self.save_path, prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(train_infos, f, indent=4)\n",
    "        with open(os.path.join(self.save_path, \"val_\" + prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(val_infos, f, indent=4)\n",
    "        \n",
    "    \n",
    "    def generation(self):\n",
    "        json_infos = []\n",
    "        val_infos = []\n",
    "        for idx, data in enumerate(self.dataset):\n",
    "            data_dict = {}\n",
    "            is_done = data[\"terminated\"] or data[\"truncated\"]\n",
    "            camera = data[\"obs\"]['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            hand_camera = data[\"obs\"]['sensor_data'][\"hand_camera\"][\"rgb\"]\n",
    "            qpos = data[\"obs\"][\"agent\"][\"qpos\"]\n",
    "            tcp_pose = data[\"obs\"][\"extra\"][\"tcp_pose\"]\n",
    "            action = data[\"action\"]\n",
    "            rescaled_qpos = self.rescale(qpos, 'qpos')\n",
    "            rescaled_tcp_pose = self.rescale(tcp_pose, 'tcp_pose')\n",
    "            rescaled_action = self.rescale(action, 'action')\n",
    "            if not os.path.exists(os.path.join(self.img_save_path, f\"{idx}.jpg\")):\n",
    "                img = Image.fromarray(camera)\n",
    "                img.save(os.path.join(self.img_save_path, f\"{idx}.jpg\"))\n",
    "            if not os.path.exists(os.path.join(self.img_save_path, f\"{idx}_hand.jpg\")):\n",
    "                hand_img = Image.fromarray(hand_camera)\n",
    "                hand_img.save(os.path.join(self.img_save_path, f\"{idx}_hand.jpg\"))\n",
    "            \n",
    "            # query = f\"The current joint state of the robotic arm is as follows: {{{rescaled_qpos[0]} {rescaled_qpos[1]} {rescaled_qpos[2]} {rescaled_qpos[3]} {rescaled_qpos[4]} {rescaled_qpos[5]} {rescaled_qpos[6]} {rescaled_qpos[7]} {rescaled_qpos[8]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "            # query = f\"The current position state of the robotic arm's end gripper is as follows: {{Joint_0: {rescaled_qpos[0]}, Joint_1: {rescaled_qpos[1]}, Joint_2: {rescaled_qpos[2]}, Joint_3: {rescaled_qpos[3]}, Joint_4: {rescaled_qpos[4]}, Joint_5: {rescaled_qpos[5]}, Joint_6: {rescaled_qpos[6]}, Joint_7: {rescaled_qpos[7]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "            query = f\"Based on whether the robot arm's gripper successfully grasps the object and the distance between the robot arm's endpoint and the target position in the image, provide a comprehensive rating, where a higher score indicates better task completion (max score 900). Please rate the completion of the robot arm with gripper follows instruction: sweep the trash to the white bin.\\nThe current position state of the robotic arm's end gripper is as follows: {{x: {eef_xyz[0]}mm, y: {eef_xyz[1]}mm, z: {eef_xyz[2]}mm, roll: {eef_rpy[0]} degrees, pitch: {eef_rpy[1]} degrees, yaw: {eef_rpy[2]} degrees, open: {gripper_state}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "            action_str = f\"action: {{x: {rescaled_action[0]}mm, y: {rescaled_action[1]}mm, z: {rescaled_action[2]}mm, roll: {rescaled_action[3]} degrees, pitch: {rescaled_action[4]} degrees, yaw: {rescaled_action[5]} degrees, open: {rescaled_action[6]}}}\"\n",
    "            # action_str = \" \".join([f\"{str(int(a))}\" for a in rescaled_action])\n",
    "            # action_str = '{' + action_str + '}'\n",
    "            data_dict['query'] = query\n",
    "            data_dict['response'] = action_str\n",
    "            data_dict['images'] = [os.path.join(self.img_save_path, f\"{idx}.jpg\"), os.path.join(self.img_save_path, f\"{idx}_hand.jpg\")]\n",
    "            if idx in self.val_ids:\n",
    "                val_infos.append(data_dict)\n",
    "            else:\n",
    "                json_infos.append(data_dict)\n",
    "        if self.dual_camera:\n",
    "            prefix = \"dualcam_\"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "        with open(os.path.join(self.save_path, prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(json_infos, f, indent=4)\n",
    "        with open(os.path.join(self.save_path, \"val_\" + prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(val_infos, f, indent=4)\n",
    "            \n",
    "\n",
    "class InternVLPretrainDatasetGeneratorNew:\n",
    "    def __init__(self, dataset: ManiSkillTrajectoryDataset, save_path: str, horizon=1, dual_camera=False, is_joint_action=False, env_id=\"StackCube-v1\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset generator for InternVL pretraining.\n",
    "\n",
    "        Args:\n",
    "            dataset_path (str): Path to the dataset file.\n",
    "            load_count (int): Number of trajectories to load. If -1, loads all.\n",
    "            success_only (bool): Whether to filter for successful trajectories only.\n",
    "            device: Device to load data onto (e.g., 'cpu', 'cuda').\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.save_path = save_path\n",
    "        self.img_save_path = os.path.join(save_path, \"images\")\n",
    "        os.makedirs(self.img_save_path, exist_ok=True)\n",
    "        rng = np.random.default_rng(0)\n",
    "        num_steps = np.sum([len(act) for act in dataset.actions])\n",
    "        self.val_ids = set(rng.choice(num_steps, size=num_steps // 20, replace=False))\n",
    "        self.rng = rng\n",
    "        self.rescale_array = {\n",
    "            'action': np.array([1000] * 7 + [1]) if is_joint_action else np.array([1000, 1000, 1000, 57.3, 57.3, 57.3] + [1]),\n",
    "            # 'action': np.array([1000] * 7 + [1]) if is_joint_action else np.array([1000, 1000, 1000, 1000, 1000, 1000] + [1]),\n",
    "            'qpos': np.array([1000] * 9),\n",
    "            'tcp_pose': np.array([1000] * 7)\n",
    "        }\n",
    "        self.horizon = horizon\n",
    "        self.dual_camera = dual_camera\n",
    "        self.instruction = INSTRUCTIONS[env_id]\n",
    "        \n",
    "    def cal_statistics(self):\n",
    "        statistics = {\n",
    "            'action': {},\n",
    "            'qpos': {},\n",
    "            'tcp_pose': {}\n",
    "        }\n",
    "        all_collections = {\n",
    "            'action': [],\n",
    "            'qpos': [],\n",
    "            'tcp_pose': []\n",
    "        }\n",
    "        for data in self.dataset:\n",
    "            # camera = data['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            qpos = data['obs'][\"agent\"][\"qpos\"]\n",
    "            tcp_pose = data['obs'][\"extra\"][\"tcp_pose\"]\n",
    "            rpy = quat_to_rpy(tcp_pose[3:7], degrees=True)\n",
    "            tcp_pose = np.concatenate([tcp_pose[:3], rpy])\n",
    "            action = data[\"action\"]\n",
    "            all_collections['action'].append(action)\n",
    "            all_collections['qpos'].append(qpos)\n",
    "            all_collections['tcp_pose'].append(tcp_pose)\n",
    "\n",
    "        for key in statistics.keys():\n",
    "            all_data = all_collections[key]\n",
    "            statistics[key]['mean'] = np.mean(all_data, axis=0).tolist()\n",
    "            statistics[key]['std'] = np.std(all_data, axis=0).tolist()\n",
    "            statistics[key]['min'] = np.min(all_data, axis=0).tolist()\n",
    "            statistics[key]['max'] = np.max(all_data, axis=0).tolist()\n",
    "        # print(\"Statistics calculated:\", statistics)\n",
    "        self.statistics = statistics\n",
    "        with open(os.path.join(self.save_path, 'statistics.json'), 'w') as f:\n",
    "            json.dump(statistics, f, indent=4)\n",
    "     \n",
    "    def rescale(self, data, key):\n",
    "        _tmp_data = np.round(data * self.rescale_array[key]).astype(np.int32)\n",
    "        if key == \"action\":\n",
    "            _tmp_data = np.clip(-999, 999, _tmp_data)\n",
    "        return _tmp_data\n",
    "    \n",
    "    def process_episode_data(self, episode_data, filter_zero=False):\n",
    "        infos = []\n",
    "        for i in range(len(episode_data['queries'])):\n",
    "            if self.horizon > 1:\n",
    "                if i + self.horizon > len(episode_data['queries']):\n",
    "                    responses = \" \".join(episode_data['action_strs'][i:])\n",
    "                    delta = i + self.horizon - len(episode_data['queries'])\n",
    "                    responses += f\" +0 +0 +0 +0 +0 +0 {episode_data['action_strs'][-1][-3:-1]}|\" * delta\n",
    "                else:\n",
    "                    responses = \" \".join(episode_data['action_strs'][i:i+self.horizon])\n",
    "            else:\n",
    "                responses = episode_data['action_strs'][i]\n",
    "                if filter_zero:\n",
    "                    if responses[:12] == \" 0 0 0 0 0 0\":\n",
    "                        continue\n",
    "            data_dict = {\n",
    "                'query': episode_data['queries'][i],\n",
    "                'response': responses,\n",
    "                'images': episode_data['cameras_save_path'][i]\n",
    "            }\n",
    "            infos.append(data_dict)\n",
    "        return infos\n",
    "\n",
    "    def traj_generation(self, filter_zero=False):\n",
    "        all_infos = []\n",
    "        val_infos = []\n",
    "        train_infos = []\n",
    "        for episode_idx, (obs, action, terminated, truncated) in enumerate(self.dataset):\n",
    "            episode_data = {\n",
    "                \"queries\": [],\n",
    "                \"action_strs\": [],\n",
    "                \"cameras_save_path\": [],\n",
    "            }\n",
    "            cameras = obs['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            if \"hand_camera\" in obs['sensor_data']:\n",
    "                hand_cameras = obs['sensor_data'][\"hand_camera\"][\"rgb\"]\n",
    "            else:\n",
    "                hand_cameras = [None] * len(cameras)\n",
    "            qposes = obs[\"agent\"][\"qpos\"]\n",
    "            tcp_pose = obs[\"extra\"][\"tcp_pose\"]\n",
    "            rescaled_qposes = self.rescale(qposes, 'qpos')\n",
    "            # rescaled_tcp_poses = self.rescale(tcp_poses, 'tcp_pose')\n",
    "            rescaled_actions = self.rescale(action, 'action')\n",
    "            rpys = []\n",
    "            for quat in tcp_pose[:, 3:7]:\n",
    "                rpy = quat_to_rpy(quat, degrees=True)\n",
    "                rpys.append(rpy)\n",
    "            rpys = np.array(rpys)\n",
    "            tcp_pose = np.concatenate([tcp_pose[:, :3], rpys], axis=-1)\n",
    "            tcp_pose[:, :3] = np.round(tcp_pose[:, :3] * 1000).astype(np.int32)\n",
    "            tcp_pose[:, 3:] = np.round(tcp_pose[:, 3:]).astype(np.int32)\n",
    "            len_episode = len(cameras)\n",
    "            for local_step, (camera, hand_camera, rescaled_action, rescaled_qpos, rescaled_tcp_pose) in enumerate(zip(cameras, hand_cameras, rescaled_actions, rescaled_qposes, tcp_pose)): \n",
    "                if np.all(rescaled_action[:6] == 0):\n",
    "                    continue\n",
    "                camera_save_path = os.path.join(self.img_save_path, f\"{episode_idx}_{local_step}_0.jpg\")\n",
    "                hand_camera_save_path = os.path.join(self.img_save_path, f\"{episode_idx}_{local_step}_1.jpg\")\n",
    "                camera_save_name = camera_save_path\n",
    "                hand_camera_save_name = hand_camera_save_path\n",
    "                if not os.path.exists(camera_save_path):\n",
    "                    img = Image.fromarray(camera)\n",
    "                    img.save(camera_save_path)\n",
    "                if not os.path.exists(hand_camera_save_path) and hand_camera is not None:\n",
    "                    hand_img = Image.fromarray(hand_camera)\n",
    "                    hand_img.save(hand_camera_save_path)\n",
    "                if rescaled_qpos[-1] >= 37:\n",
    "                    gripper_state = 1\n",
    "                else:\n",
    "                    gripper_state = 0\n",
    "                eef_xyz = rescaled_tcp_pose[:3]\n",
    "                eef_rpy = rescaled_tcp_pose[3:]\n",
    "                # query = f\"The current joint state of the robotic arm is as follows: {{{rescaled_qpos[0]} {rescaled_qpos[1]} {rescaled_qpos[2]} {rescaled_qpos[3]} {rescaled_qpos[4]} {rescaled_qpos[5]} {rescaled_qpos[6]} {rescaled_qpos[7]} {rescaled_qpos[8]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                query = f\"Based on whether the robot arm's gripper successfully grasps the object and the distance between the robot arm's endpoint and the target position in the image, provide a comprehensive rating, where a higher score indicates better task completion (max score 900). Please rate the completion of the robot arm with gripper follows instruction: sweep the trash to the white bin.\\nThe current position state of the robotic arm's end gripper is as follows: {{x: {eef_xyz[0]}mm, y: {eef_xyz[1]}mm, z: {eef_xyz[2]}mm, roll: {eef_rpy[0]} degrees, pitch: {eef_rpy[1]} degrees, yaw: {eef_rpy[2]} degrees, open: {gripper_state}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                # joints_str = \", \".join(f\"Joint_{i}: {v}\" for i, v in enumerate(rescaled_qpos[:8]))\n",
    "                # query = f\"The current position state of the robotic arm's end gripper is as follows: {{{joints_str}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "                \n",
    "                # action_str = \" \".join([f\"{str(int(a))}\" for a in rescaled_action])\n",
    "                # action_str = action_str\n",
    "                rescaled_action[-1] = 0 if rescaled_action[-1] == -1 else 1\n",
    "                action_str = f\"completion value:{901 - len_episode + local_step}\\naction: {{x: {rescaled_action[0]}mm, y: {rescaled_action[1]}mm, z: {rescaled_action[2]}mm, roll: {rescaled_action[3]} degrees, pitch: {rescaled_action[4]} degrees, yaw: {rescaled_action[5]} degrees, open: {rescaled_action[6]}}}\"\n",
    "                # if rescaled_action[0] >= 0:\n",
    "                #     action_str = \" \" + action_str\n",
    "                episode_data['queries'].append(query)\n",
    "                episode_data['action_strs'].append(action_str)\n",
    "                if self.dual_camera:\n",
    "                    camera_save_names = [camera_save_name, hand_camera_save_name]\n",
    "                else: \n",
    "                    camera_save_names = [camera_save_name]\n",
    "                episode_data['cameras_save_path'].append(camera_save_names)\n",
    "            infos = self.process_episode_data(episode_data, filter_zero)\n",
    "            all_infos.extend(infos)\n",
    "        if self.dual_camera:\n",
    "            prefix = \"dualcam_\"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "        for idx in range(len(all_infos)):\n",
    "            if idx in self.val_ids:\n",
    "                val_infos.append(all_infos[idx])\n",
    "            else:\n",
    "                train_infos.append(all_infos[idx])\n",
    "        with open(os.path.join(self.save_path, prefix + f'new_dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(train_infos, f, indent=4)\n",
    "        with open(os.path.join(self.save_path, \"val_\" + prefix + f'new_dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(val_infos, f, indent=4)\n",
    "        \n",
    "    \n",
    "    def generation(self):\n",
    "        json_infos = []\n",
    "        val_infos = []\n",
    "        for idx, data in enumerate(self.dataset):\n",
    "            data_dict = {}\n",
    "            is_done = data[\"terminated\"] or data[\"truncated\"]\n",
    "            camera = data[\"obs\"]['sensor_data'][\"base_camera\"][\"rgb\"]\n",
    "            hand_camera = data[\"obs\"]['sensor_data'][\"hand_camera\"][\"rgb\"]\n",
    "            qpos = data[\"obs\"][\"agent\"][\"qpos\"]\n",
    "            tcp_pose = data[\"obs\"][\"extra\"][\"tcp_pose\"]\n",
    "            action = data[\"action\"]\n",
    "            rescaled_qpos = self.rescale(qpos, 'qpos')\n",
    "            rescaled_tcp_pose = self.rescale(tcp_pose, 'tcp_pose')\n",
    "            rescaled_action = self.rescale(action, 'action')\n",
    "            if not os.path.exists(os.path.join(self.img_save_path, f\"{idx}.jpg\")):\n",
    "                img = Image.fromarray(camera)\n",
    "                img.save(os.path.join(self.img_save_path, f\"{idx}.jpg\"))\n",
    "            if not os.path.exists(os.path.join(self.img_save_path, f\"{idx}_hand.jpg\")):\n",
    "                hand_img = Image.fromarray(hand_camera)\n",
    "                hand_img.save(os.path.join(self.img_save_path, f\"{idx}_hand.jpg\"))\n",
    "            \n",
    "            # query = f\"The current joint state of the robotic arm is as follows: {{{rescaled_qpos[0]} {rescaled_qpos[1]} {rescaled_qpos[2]} {rescaled_qpos[3]} {rescaled_qpos[4]} {rescaled_qpos[5]} {rescaled_qpos[6]} {rescaled_qpos[7]} {rescaled_qpos[8]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "            query = f\"The current position state of the robotic arm's end gripper is as follows: {{Joint_0: {rescaled_qpos[0]}, Joint_1: {rescaled_qpos[1]}, Joint_2: {rescaled_qpos[2]}, Joint_3: {rescaled_qpos[3]}, Joint_4: {rescaled_qpos[4]}, Joint_5: {rescaled_qpos[5]}, Joint_6: {rescaled_qpos[6]}, Joint_7: {rescaled_qpos[7]}}}. What action should the robot take to get better completion of instruction: {self.instruction}?\"\n",
    "\n",
    "            action_str = \" \".join([f\"{str(int(a))}\" for a in rescaled_action])\n",
    "            action_str = '{' + action_str + '}'\n",
    "            data_dict['query'] = query\n",
    "            data_dict['response'] = action_str\n",
    "            data_dict['images'] = [os.path.join(self.img_save_path, f\"{idx}.jpg\"), os.path.join(self.img_save_path, f\"{idx}_hand.jpg\")]\n",
    "            if idx in self.val_ids:\n",
    "                val_infos.append(data_dict)\n",
    "            else:\n",
    "                json_infos.append(data_dict)\n",
    "        if self.dual_camera:\n",
    "            prefix = \"dualcam_\"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "        with open(os.path.join(self.save_path, prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(json_infos, f, indent=4)\n",
    "        with open(os.path.join(self.save_path, \"val_\" + prefix + f'dataset_{self.horizon}.json'), 'w') as f:\n",
    "            json.dump(val_infos, f, indent=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InternVLPretrainDatasetGenerator(\n",
    "    dataset=eef_sq_ds,\n",
    "    save_path=\"push_cubes\",\n",
    "    horizon=1,\n",
    "    dual_camera=False,\n",
    "    is_joint_action=False,\n",
    "    )\n",
    "# generator.cal_statistics()\n",
    "generator.traj_generation()\n",
    "# generator.generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mani_skill.envs\n",
    "import gymnasium as gym\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Set to the GPU you want to use\n",
    "\n",
    "N = 256\n",
    "env = gym.make(\"PickCube-v1\", num_envs=N, reconfiguration_freq=None)\n",
    "env = ManiSkillVectorEnv(env, num_envs=N, ignore_terminations=False, auto_reset=True, record_metrics=True)\n",
    "env.action_space # shape (N, D)\n",
    "env.single_action_space # shape (D, )\n",
    "env.observation_space # shape (N, ...)\n",
    "env.single_observation_space # shape (...)\n",
    "env.reset()\n",
    "obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "# obs (N, ...), rew (N, ), terminated (N, ), truncated (N, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([True, True, True, True], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([False, False, False, False], device='cuda:0')\n",
      "tensor([False, False, False, False], device='cuda:0') tensor([True, True, True, True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import mani_skill.envs\n",
    "import gymnasium as gym\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "N = 4\n",
    "env = gym.make(\"PickCube-v1\", num_envs=N, max_episode_steps=50)\n",
    "env = ManiSkillVectorEnv(env, auto_reset=True, ignore_terminations=False)\n",
    "env.action_space # shape (N, D)\n",
    "env.single_action_space # shape (D, )\n",
    "env.observation_space # shape (N, ...)\n",
    "env.single_observation_space # shape (...)\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    print(terminated, truncated)\n",
    "# obs (N, ...), rew (N, ), terminated (N, ), truncated (N, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `PushCube` doesn't exist. Did you mean: `Pusher`?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m num_eval_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      7\u001b[0m env_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(obs_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# modify your env_kwargs here\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m eval_envs \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_eval_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconfiguration_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# add any other wrappers here\u001b[39;00m\n\u001b[1;32m     10\u001b[0m eval_envs \u001b[38;5;241m=\u001b[39m ManiSkillVectorEnv(eval_envs, ignore_terminations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/gymnasium/envs/registration.py:741\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/gymnasium/envs/registration.py:527\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    521\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/gymnasium/envs/registration.py:393\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/gymnasium/envs/registration.py:370\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `PushCube` doesn't exist. Did you mean: `Pusher`?"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from collections import defaultdict\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "env_id = \"PushCube-v1\"\n",
    "num_eval_envs = 64\n",
    "env_kwargs = dict(obs_mode=\"rgb\") # modify your env_kwargs here\n",
    "eval_envs = gym.make(env_id, num_envs=num_eval_envs, reconfiguration_freq=None, **env_kwargs)\n",
    "# add any other wrappers here\n",
    "eval_envs = ManiSkillVectorEnv(eval_envs, ignore_terminations=False, record_metrics=True)\n",
    "\n",
    "# evaluation loop, which will record metrics for complete episodes only\n",
    "obs, _ = eval_envs.reset(seed=0)\n",
    "eval_metrics = defaultdict(list)\n",
    "for _ in range(450):\n",
    "    action = eval_envs.action_space.sample() # replace with your policy action\n",
    "    obs, rew, terminated, truncated, info = eval_envs.step(action)\n",
    "    # note as there are no partial resets, truncated is True for all environments at the same time\n",
    "    print(\"step\")\n",
    "    if truncated.any() or terminated.any():\n",
    "        for k, v in info[\"final_info\"][\"episode\"].items():\n",
    "            eval_metrics[k].append(v.float())\n",
    "for k in eval_metrics.keys():\n",
    "    print(f\"{k}_mean: {torch.mean(torch.stack(eval_metrics[k])).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_once_mean: 0.0\n",
      "return_mean: 7.305637359619141\n",
      "episode_len_mean: 150.0\n",
      "reward_mean: 0.048704247921705246\n",
      "success_at_end_mean: 0.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in eval_metrics.items():\n",
    "    print(f\"{k}_mean: {torch.mean(torch.stack(v)).item()}\")\n",
    "    eval_metrics[k] = torch.stack(v).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics[k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing video: ./test_save/0_0_0_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/1_1_0_150_False.mp4, Num steps: 150, Success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0 done with reward 0.0027087628841400146, num dones: 16\n",
      "Environment 1 done with reward 0.018450289964675903, num dones: 15\n",
      "Writing video: ./test_save/2_0_1_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/3_1_1_150_False.mp4, Num steps: 150, Success: False\n",
      "Environment 0 done with reward 0.0006744414567947388, num dones: 14\n",
      "Environment 1 done with reward 0.007344335317611694, num dones: 13\n",
      "Writing video: ./test_save/4_0_2_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/5_1_2_150_False.mp4, Num steps: 150, Success: False\n",
      "Environment 0 done with reward 0.0005099326372146606, num dones: 12\n",
      "Environment 1 done with reward 0.01807159185409546, num dones: 11\n",
      "Writing video: ./test_save/6_0_3_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/7_1_3_150_False.mp4, Num steps: 150, Success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0 done with reward 0.009738549590110779, num dones: 10\n",
      "Environment 1 done with reward 0.002504289150238037, num dones: 9\n",
      "Writing video: ./test_save/8_0_4_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/9_1_4_150_False.mp4, Num steps: 150, Success: False\n",
      "Environment 0 done with reward 0.0008889734745025635, num dones: 8\n",
      "Environment 1 done with reward 0.0012055039405822754, num dones: 7\n",
      "Writing video: ./test_save/10_0_5_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/11_1_5_150_False.mp4, Num steps: 150, Success: False\n",
      "Environment 0 done with reward 0.004766255617141724, num dones: 6\n",
      "Environment 1 done with reward 6.490945816040039e-05, num dones: 5\n",
      "Writing video: ./test_save/12_0_6_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/13_1_6_150_False.mp4, Num steps: 150, Success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0 done with reward 0.003990381956100464, num dones: 4\n",
      "Environment 1 done with reward 0.005703866481781006, num dones: 3\n",
      "Writing video: ./test_save/14_0_7_150_False.mp4, Num steps: 150, Success: False\n",
      "Writing video: ./test_save/15_1_7_150_False.mp4, Num steps: 150, Success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 529) to (480, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0 done with reward 0.049762338399887085, num dones: 2\n",
      "Environment 1 done with reward 0.010378226637840271, num dones: 1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import mani_skill.envs\n",
    "from mani_skill.utils.wrappers.gymnasium import CPUGymWrapper\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "\n",
    "num_envs = 2\n",
    "env = gym.make(\n",
    "    \"StackCube-v1\", # there are more tasks e.g. \"PushCube-v1\", \"PegInsertionSide-v1\", ...\n",
    "    num_envs=num_envs,\n",
    "    obs_mode=\"rgb\", # there is also \"state_dict\", \"rgbd\", ...\n",
    "    control_mode=\"pd_ee_delta_pose\", # there is also \"pd_joint_delta_pos\", ...\n",
    "    sensor_configs={'height': 480, 'width': 480}, # camera configs\n",
    "    max_episode_steps=150,\n",
    ")\n",
    "env = ManiSkillVectorEnv(env, auto_reset=True, ignore_terminations=False)\n",
    "# print(\"Observation space\", env.observation_space)\n",
    "# print(\"Action space\", env.action_space)\n",
    "# env = CPUGymWrapper(env)  # wrap to use CPU for rendering\n",
    "obs, _ = env.reset(seed=0) # reset with a seed for determinism\n",
    "last_obs = obs\n",
    "num_dones = 16\n",
    "from internvl_eval.InternVL_eval_agent import VideoRecorder\n",
    "recorder = VideoRecorder(\n",
    "    save_path=\"./test_save\",\n",
    "    fps=30,\n",
    "    num_envs=num_envs,\n",
    ")\n",
    "while num_dones > 0:\n",
    "    actions = env.action_space.sample()  # sample random actions\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)  # step with\n",
    "    is_terminals = np.logical_or(terminated.cpu().numpy(), truncated.cpu().numpy())\n",
    "    recorder.append_obs(last_obs, info['success'].cpu().numpy(), is_terminals, actions)\n",
    "    last_obs = obs\n",
    "    done_env_ids = np.where(is_terminals)[0]\n",
    "    if len(done_env_ids) > 0:\n",
    "        obs, _ = env.reset(options={\"env_idx\": done_env_ids, \"reconfigure\": True})  # reset the environments that are done\n",
    "        last_obs = obs\n",
    "    for env_id in done_env_ids:\n",
    "        print(f\"Environment {env_id} done with reward {reward[env_id]}, num dones: {num_dones}\")\n",
    "        num_dones -= 1\n",
    "# done = False\n",
    "# Image.fromarray(obs[\"sensor_data\"]['base_camera']['rgb']).show()  # show the first environment's RGB observation\n",
    "# while not done:\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     done = terminated or truncated\n",
    "#     # env.render()  # a display is required to render\n",
    "# env.close()\n",
    "\n",
    "\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/StackCube-v1/motionplanning/trajectory.h5 --obs_mode rgb --target_control_mode pd_ee_delta_pose --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 100\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/PickCube-v1/motionplanning/trajectory.h5 --obs_mode rgb --target_control_mode pd_ee_delta_pose --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 100\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/PickCube-v1/motionplanning/trajectory.h5 --obs_mode rgb --target_control_mode pd_joint_delta_pos --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 100\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/PushCube-v1/motionplanning/trajectory.h5 --obs_mode rgb --target_control_mode pd_ee_delta_pose --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 500\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/StackCube-v1/rl/trajectory.none.pd_ee_delta_pose.physx_cuda.h5 --obs_mode rgb --target_control_mode pd_ee_delta_pose --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 300\n",
    "# python -m mani_skill.trajectory.replay_trajectory --traj_path demos/StackCube-v1/motionplanning/trajectory.h5 --obs_mode rgb --target_control_mode pd_joint_delta_pos --verbose --save_traj --max_retry 2 --reward_mode 'sparse' --record_rewards --num_envs 32 --count 100\n",
    "# CUDA_VISIBLE_DEVICES=0 python train_rgbd.py --env-id StackCube-v1     --demo-path demos/StackCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5     --control-mode \"pd_ee_delta_pose\" --sim-backend \"physx_cpu\" --num-demos 100 --max_episode_steps 200     --total_iters 100000 --obs-mode \"rgb\"     --exp-name diffusion_policy-StackCube-v1-rgb-100_motionplanning_demos-1     --demo_type=motionplanning --track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ds_json_path = \"ManiSkill_Demonstrations/demos/PickCube-v1/teleop/trajectory.json\"\n",
    "with open(ds_json_path, \"r\") as f:\n",
    "    env_info = json.load(f)\n",
    "env = gym.make(env_info[\"env_id\"], **env_info[\"env_kwargs\"])\n",
    "episode = env_info[\"episodes\"][0] # picks the first\n",
    "env.reset(**episode[\"reset_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/sapien/_vulkan_tricks.py:21: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n",
      "/mnt/nfs3/caozhe/miniconda3/envs/verl/lib/python3.10/site-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find Vulkan ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from mani_skill.utils import sapien_utils\n",
    "from mani_skill.utils import common\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_h5_data(data):\n",
    "    out = dict()\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], h5py.Dataset):\n",
    "            out[k] = data[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(data[k])\n",
    "    return out\n",
    "\n",
    "def to_tensors(x, device=None):\n",
    "    \"\"\"\n",
    "    Converts numpy arrays or dicts of numpy arrays to torch tensors.\n",
    "    If device is specified, moves the tensors to that device.\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        return {k: to_tensors(v, device) for k, v in x.items()}\n",
    "    elif isinstance(x, np.ndarray) and device is not None:\n",
    "        tensor = torch.as_tensor(x).to(device)\n",
    "        return tensor\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "class ManiSkillTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
    "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
    "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
    "\n",
    "    Args:\n",
    "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
    "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
    "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
    "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_file: str, load_count=-1, success_only: bool = False, device = None) -> None:\n",
    "        self.dataset_file = dataset_file\n",
    "        self.device = device\n",
    "        self.data = h5py.File(dataset_file, \"r\")\n",
    "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "        self.json_data = load_json(json_path)\n",
    "        self.episodes = self.json_data[\"episodes\"]\n",
    "        self.env_info = self.json_data[\"env_info\"]\n",
    "        self.env_id = self.env_info[\"env_id\"]\n",
    "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
    "        if isinstance(load_count, int):\n",
    "            self.load_dataset(load_count, success_only, device)\n",
    "        else:\n",
    "            pass    \n",
    "        \n",
    "    def load_dataset(self, load_count, success_only, device):\n",
    "        self.obs = None\n",
    "        self.actions = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "        self.success, self.fail, self.rewards = None, None, None\n",
    "        if load_count == -1:\n",
    "            load_count = len(self.episodes)\n",
    "        for eps_id in tqdm(range(load_count)):\n",
    "            eps = self.episodes[eps_id]\n",
    "            if success_only: \n",
    "                assert \"success\" in eps, \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
    "                if not eps[\"success\"]:\n",
    "                    continue\n",
    "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
    "            trajectory = load_h5_data(trajectory)\n",
    "            eps_len = len(trajectory[\"actions\"])\n",
    "            \n",
    "            # exclude the final observation as most learning workflows do not use it\n",
    "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "            if eps_id == 0:\n",
    "                self.obs = obs\n",
    "            else:\n",
    "                self.obs = common.append_dict_array(self.obs, obs)\n",
    "\n",
    "            self.actions.append(trajectory[\"actions\"])\n",
    "            self.terminated.append(trajectory[\"terminated\"])\n",
    "            self.truncated.append(trajectory[\"truncated\"])\n",
    "\n",
    "            # handle data that might optionally be in the trajectory\n",
    "            if \"rewards\" in trajectory:\n",
    "                if self.rewards is None:\n",
    "                    self.rewards = [trajectory[\"rewards\"]]\n",
    "                else:\n",
    "                    self.rewards.append(trajectory[\"rewards\"])\n",
    "            if \"success\" in trajectory:\n",
    "                if self.success is None:\n",
    "                    self.success = [trajectory[\"success\"]]\n",
    "                else:\n",
    "                    self.success.append(trajectory[\"success\"])\n",
    "            if \"fail\" in trajectory:\n",
    "                if self.fail is None:\n",
    "                    self.fail = [trajectory[\"fail\"]]\n",
    "                else:\n",
    "                    self.fail.append(trajectory[\"fail\"])\n",
    "\n",
    "        self.actions = np.vstack(self.actions)\n",
    "        self.terminated = np.concatenate(self.terminated)\n",
    "        self.truncated = np.concatenate(self.truncated)\n",
    "        \n",
    "        if self.rewards is not None:\n",
    "            self.rewards = np.concatenate(self.rewards)\n",
    "        if self.success is not None:\n",
    "            self.success = np.concatenate(self.success)\n",
    "        if self.fail is not None:\n",
    "            self.fail = np.concatenate(self.fail)\n",
    "\n",
    "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "        \n",
    "        # uint16 dtype is used to conserve disk space and memory\n",
    "        # you can optimize this dataset code to keep it as uint16 and process that\n",
    "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "        # it can automatically be converted to torch tensors without complaint\n",
    "        self.obs = remove_np_uint16(self.obs)\n",
    "\n",
    "        if device is not None:\n",
    "            self.actions = to_tensors(self.actions, device=device)\n",
    "            self.obs = to_tensors(self.obs, device=device)\n",
    "            self.terminated = to_tensors(self.terminated, device=device)\n",
    "            self.truncated = to_tensors(self.truncated, device=device)\n",
    "            if self.rewards is not None:\n",
    "                self.rewards = to_tensors(self.rewards, device=device)\n",
    "            if self.success is not None:\n",
    "                self.success = to_tensors(self.terminated, device=device)\n",
    "            if self.fail is not None:\n",
    "                self.fail = to_tensors(self.truncated, device=device)\n",
    "                \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        action = to_tensors(self.actions[idx], device=self.device)\n",
    "        obs = common.index_dict_array(self.obs, idx, inplace=False)\n",
    "\n",
    "        res = dict(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            terminated=self.terminated[idx],\n",
    "            truncated=self.truncated[idx],\n",
    "        )\n",
    "        if self.rewards is not None:\n",
    "            res.update(reward=self.rewards[idx])\n",
    "        if self.success is not None:\n",
    "            res.update(success=self.success[idx])\n",
    "        if self.fail is not None:\n",
    "            res.update(fail=self.fail[idx])\n",
    "        return res\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_dataset = ManiSkillTrajectoryDataset(dataset_file=\"ManiSkill_Demonstrations/demos/StackCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5\", load_count=10, success_only=False, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minternvl_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInternVL_eval_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m video_writing, write_instruction_action, action_to_str\n",
      "File \u001b[0;32m/cpfs/user/caozhe/ManiSkill/internvl_eval/InternVL_eval_agent.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# from internvl_eval.InternVL_eval_agent import video_writing, write_instruction_action, action_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (960, 529) to (960, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "# all_cameras = []\n",
    "# ep_r = 0\n",
    "# for data in small_dataset:\n",
    "#     success = data['success']\n",
    "#     reward = data['reward']\n",
    "#     ep_r += reward\n",
    "#     terminated = data['terminated']\n",
    "#     truncated = data['truncated']\n",
    "#     action = data['action']\n",
    "#     camera = data['obs']['sensor_data']['base_camera']['rgb']\n",
    "#     wrist_cam = data['obs']['sensor_data']['hand_camera']['rgb']\n",
    "#     camera = np.concatenate([camera, wrist_cam], axis=1)  # concatenate the two cameras\n",
    "#     instruction = f\"R: {np.round(reward, 3)} EPR: {np.round(ep_r, 3)} Te: {terminated} Tr: {truncated} Su: {success}\"\n",
    "#     action = f\"A: {action_to_str(action, 3)}\"\n",
    "#     camera = write_instruction_action(instruction, camera, action)\n",
    "#     all_cameras.append(camera)\n",
    "#     if terminated or truncated:\n",
    "#         ep_r = 0\n",
    "# video_writing([all_cameras], \"./test_video.mp4\", fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ManiSkillTrajectoryDataset(dataset_file=\"demos/StackCube-v1/motionplanning/trajectory.rgb.pd_ee_delta_pose.physx_cpu.h5\", success_only=False, device=None)\n",
    "generator = InternVLPretrainDatasetGenerator(\n",
    "    dataset=dataset,\n",
    "    save_path=\"stack_cubes\"\n",
    "    )\n",
    "generator.cal_statistics()\n",
    "generator.generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
